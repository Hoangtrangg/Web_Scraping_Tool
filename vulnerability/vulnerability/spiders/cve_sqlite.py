#One of the problems that we've seen with CSV and JSON is that you can but it's really hard,
# really tricky, really difficult to to keep appending things and to save your progress as you are making work,
# producing information and gathering data. That is one of the things that persistent to a database solves, 
#so we're going to use SQLite3 instead of persistent to CSV and JSON

import scrapy
import sqlite3

class CveCsvJsonSpider(scrapy.Spider):

    name = "cve_sqlite"
    allowed_domains = ["cve.mitre.org"]
    start_urls = ["https://cve.mitre.org/data/refs/refmap/source-EXPLOIT-DB.html"]

    def parse(self, response):

        # create a connection
        connection = sqlite3.connect('vulnerability.db')
        create_table = 'create table list_cve (exploit_id text, cve_id text)'
        cursor = connection.cursor()
        cursor.execute(create_table)
        connection.commit()

        for child in response.xpath('//table'):
            if len(child.xpath('//tr'))>100:
                table = child
                break

        count = 0
        for row in table.xpath('//tr'):
            if count>100:
                break
            try:
                exploit_id = row.xpath('td//text()')[0].extract()
                cve_id = row.xpath('td//text()')[2].extract()

                #insert_query = 'insert into cve(exploit_id, cve_id) values(?,?)'
                cursor.execute('insert into list_cve (exploit_id ,cve_id) values(?, ?)',(exploit_id, cve_id))
                connection.commit()
                
                count+=1
            except IndexError:
                pass

